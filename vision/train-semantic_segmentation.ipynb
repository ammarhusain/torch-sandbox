{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20209 training images\n",
      "Found 2000 validation images\n",
      "Loss weight: None\n",
      "Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "463it [05:41,  1.35it/s]"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import yaml\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from torch.utils import data\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "# Datasets\n",
    "from data_loader.mit_scene_parsing import MITSceneParsingLoader\n",
    "from data_loader.simulated_data import SimulatedDataLoader\n",
    "# Models\n",
    "from models.fcn import FCN32s\n",
    "# Loss\n",
    "from loss.cross_entropy_2d import CrossEntropy2d\n",
    "# Metrics\n",
    "from utils.metrics import MetricsComp, AverageComp\n",
    "\n",
    "def train(config, data_set):\n",
    "    # setup seeds\n",
    "    torch.manual_seed(config[\"seed\"])\n",
    "    torch.cuda.manual_seed(config[\"seed\"])\n",
    "    np.random.seed(config[\"seed\"])\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    # setup device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # setup augmentations\n",
    "    ## ah todo\n",
    "    \n",
    "    # setup dataloader\n",
    "    train_data_set = data_set(split=\"training\")\n",
    "    train_data = data.DataLoader(train_data_set, batch_size=config[\"batch_sz\"],\\\n",
    "                                 num_workers=config[\"num_workers\"], shuffle=True)\n",
    "    val_data = data.DataLoader(data_set(split=\"validation\"), batch_size=config[\"batch_sz\"],\\\n",
    "                               num_workers=config[\"num_workers\"])  \n",
    "    n_classes = train_data_set.n_classes\n",
    "    \n",
    "    # setup metrics\n",
    "    metrics_comp = MetricsComp(n_classes)\n",
    "    val_loss_avg_comp = AverageComp()\n",
    "    \n",
    "    # setup model\n",
    "    model = FCN32s(n_classes).to(device)\n",
    "    #model = torch.nn.DataParallel(model, device_ids=range(torch.cuda.device_count()))\n",
    "    \n",
    "    # setup optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "        \n",
    "    # setup learning rate scheduler (optim.lr_scheduler)\n",
    "    ## ah todo\n",
    "    \n",
    "    # setup loss\n",
    "    try:\n",
    "      loss_fn_weight = train_data_set.class_imbalance_weight.to(device)\n",
    "    except:\n",
    "      loss_fn_weight = None\n",
    "  \n",
    "    #weight = torch.tensor([1.0] +  9*[10.0]).to(device) # weight the background 10x less\n",
    "    loss_fn = CrossEntropy2d(weight=loss_fn_weight)\n",
    "    \n",
    "    # setup tensorboard writer & checkpoint dir \n",
    "    exp_name = config.get(\"exp_name\", \"\")\n",
    "    uniq_name = f\"{train_data_set.name}_{model.name}_{exp_name}_logs\"\n",
    "    writer = SummaryWriter(log_dir=\"checkpoints/\" + uniq_name)\n",
    "\n",
    "    writer.add_scalar(\"batch_size\", config[\"batch_sz\"])\n",
    "\n",
    "    i = 0\n",
    "    # Load a saved checkpoint\n",
    "    if config.get(\"resume_ckpoint\") is not None:\n",
    "        if os.path.isfile(config[\"resume_ckpoint\"]):\n",
    "            print(f\"Loading checkpoint: {config['resume_ckpoint']}\")\n",
    "            ckpoint = torch.load(config[\"resume_ckpoint\"])\n",
    "            model.load_state_dict(ckpoint[\"model_state\"])\n",
    "            optimizer.load_state_dict(ckpoint[\"optimizer_state\"])\n",
    "            i = ckpoint[\"epoch\"]\n",
    "            print(f\"Saved epoch: {i}, loss: {ckpoint['epoch_loss']}, time: {ckpoint['epoch_time']}\")\n",
    "        else:\n",
    "            raise FileNotFoundError(\"Unable to load saved checkpoint!, Quitting\")\n",
    "            \n",
    "    while i < config[\"epochs\"]:\n",
    "        i += 1\n",
    "        epoch_loss = 0\n",
    "        epoch_time = 0\n",
    "        print(f\"Epoch {i}\")\n",
    "        # Prepare for training\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        acc_gradients_batch = 0\n",
    "        \n",
    "        for b_i, (images, labels) in tqdm(enumerate(train_data)):\n",
    "            start_ts = time.time()\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            out = model(images) # [batch_sz, n_classes, H=512, W=512]\n",
    "            acc_gradients_batch += out.shape[0]\n",
    "\n",
    "            loss = loss_fn(out, labels)    \n",
    "            loss.backward()\n",
    "            \n",
    "            # See if it is time to take a gradient step.\n",
    "            if acc_gradients_batch >= 256: # gradient step every N samples\n",
    "              optimizer.step()\n",
    "              optimizer.zero_grad()\n",
    "              acc_gradients_batch = 0\n",
    "              \n",
    "            epoch_loss += float(loss.item())\n",
    "            epoch_time += time.time() - start_ts\n",
    "            #writer.add_scalar(f\"{i}_batch_loss\", loss.item(), b_i)\n",
    "        \n",
    "        print(f\"Epoch Loss: {epoch_loss}\")\n",
    "        writer.add_scalar(f\"epoch_loss\", epoch_loss, i)\n",
    "\n",
    "        # Run through validation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            ctr = 0\n",
    "            for _, (images_val, labels_val) in tqdm(enumerate(val_data)):\n",
    "                if ctr > 10:\n",
    "                    break\n",
    "                ctr += 1\n",
    "                images_val = images_val.to(device)\n",
    "                labels_val = labels_val.to(device)               \n",
    "                out = model(images_val) # [batch_sz, n_classes, H=512, W=512]\n",
    "                val_loss = loss_fn(out, labels_val)\n",
    "                val_loss_avg_comp.update(val_loss.item())\n",
    "                \n",
    "                _, pred = out.max(1)\n",
    "\n",
    "                pred_uv, pred_uc = np.unique(pred.cpu().numpy(), return_counts=True)\n",
    "                ps = [f\"{p_uv} ({p_uc})\" for p_uv, p_uc in zip(pred_uv, pred_uc)]\n",
    "                print(f\"pred: {ps}\")\n",
    "                \n",
    "                lbl_uv, lbl_uc = np.unique(labels_val.cpu().numpy(), return_counts=True)\n",
    "                lbls = [f\"{l_uv} ({l_uc})\" for l_uv, l_uc in zip(lbl_uv, lbl_uc)]\n",
    "                print(f\"labels_val: {lbls}\")\n",
    "\n",
    "                metrics_comp.update(label_trues=labels_val.cpu().numpy(), label_preds=pred.cpu().numpy())\n",
    "        # Add validation results to tensorboard writer\n",
    "        writer.add_scalar(\"val_loss\", val_loss_avg_comp.avg, i)\n",
    "        overall_scores, class_iou = metrics_comp.get_results()\n",
    "        print(f\"Scores after epoch {i}: {overall_scores}\")\n",
    "        for k, v in overall_scores.items():\n",
    "            writer.add_scalar(f\"val_metrics/{k}\", v, i)\n",
    "        for k, v in class_iou.items():\n",
    "            writer.add_scalar(f\"val_metrics/cls_iou_{k}\", v, i)\n",
    "        \n",
    "        if (config[\"save_ckpoint\"]):\n",
    "          # Save the model checkpoint\n",
    "          ckpoint = {\"epoch\":i,\n",
    "                     \"model_state\": model.state_dict(),\n",
    "                     \"optimizer_state\": optimizer.state_dict(),\n",
    "                     \"epoch_loss\": epoch_loss,\n",
    "                     \"epoch_time\": epoch_time,\n",
    "                    }\n",
    "          ckpoint_name = f\"checkpoints/{uniq_name}_{i%3}.pkl\"\n",
    "          torch.save(ckpoint, ckpoint_name)\n",
    "    \n",
    "#TMP batch size reduced to 8->4            \n",
    "config = {\"exp_name\": \"pad_fix\", \"batch_sz\": 8, \"epochs\": 500,\\\n",
    "          \"seed\": 3642, \"num_workers\": 1, \"save_ckpoint\": True}#, \"resume_ckpoint\": \"checkpoints/mit_sceneparsing_FCN32s_full_logs_0.pkl\", }\n",
    "\n",
    "#train(config, SimulatedDataLoader)\n",
    "train(config, MITSceneParsingLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -alh checkpoints/logs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
